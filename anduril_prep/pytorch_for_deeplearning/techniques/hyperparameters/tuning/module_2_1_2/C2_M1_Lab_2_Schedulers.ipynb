{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28341014",
   "metadata": {},
   "source": [
    "# Schedulers in PyTorch\n",
    "\n",
    "Welcome to this exploration of learning rate schedulers in PyTorch! As you have learned in the lectures, managing learning rates is vital to optimizing model training, and PyTorch provides various tools to streamline this process. Learning rate schedulers are essential for dynamically adjusting learning rates according to the training phase, helping to facilitate faster convergence and improve generalization.\n",
    "\n",
    "In this notebook, you will dive into the mechanics of different learning rate schedulers. Specifically, you will:\n",
    "\n",
    "* Explore how schedulers like `StepLR`, `CosineAnnealingLR`, and `ReduceLROnPlateau` adjust the learning rate to maintain training efficiency.\n",
    "\n",
    "* Use these schedulers within a typical training loop.\n",
    "\n",
    "* Learn to visualize training dynamics, demonstrating the impact of schedulers on model performance compared to a constant learning rate.\n",
    "\n",
    "By the end of this notebook, you will have practical experience in applying learning rate schedulers, enhancing your understanding of their role in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ffc76-5297-4c5f-8349-e05a5c4d42bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Redirect stderr to a black hole to catch other potential messages\n",
    "class BlackHole:\n",
    "    def write(self, message):\n",
    "        pass\n",
    "    def flush(self):\n",
    "        pass\n",
    "sys.stderr = BlackHole()\n",
    "\n",
    "# Ignore Python-level UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8dcf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "helper_utils.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e6227-ea9f-4d7f-8067-4bc232b4d6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Check device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700851b8",
   "metadata": {},
   "source": [
    "## A deeper dive into the training process\n",
    "\n",
    "In this section you will make a deeper analysis of the experiments we have done in the previous lab. \n",
    "You will use a simple model (`SimpleCNN`) and the CIFAR-10 dataset, but now we will inspect the training process more closely by plotting the **training loss** and **validation accuracy** curves.\n",
    "\n",
    "This will help use understand how the model learns over time and how different learning rates affect the training dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dfae8e",
   "metadata": {},
   "source": [
    "In the following cell we find `SimpleCNN` model definition, and `evaluate_epoch` function that you can use to evaluate the model on the validation set *after each epoch*.\n",
    "`train_and_evaluate` function is provided to train the model for a specified number of epochs and evaluate it during training.\n",
    "\n",
    "`get_data_loaders`and `train_epoch` functions are provided from `helper_utils` module. \n",
    "They are used to load the CIFAR-10 dataset and train the model for one epoch, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cffca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"A simple Convolutional Neural Network (CNN) architecture.\n",
    "\n",
    "    This class defines a two-layer CNN with max pooling, dropout, and\n",
    "    fully connected layers, suitable for basic image classification tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the layers of the neural network.\"\"\"\n",
    "        # Initialize the parent nn.Module class\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # First convolutional layer (3 input channels, 16 output channels, 3x3 kernel)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        # Second convolutional layer (16 input channels, 32 output channels, 3x3 kernel)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        # Max pooling layer with a 2x2 window and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # First fully connected (linear) layer\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 64)\n",
    "        # Second fully connected (linear) layer, serving as the output layer\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, 3, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output logits from the network.\n",
    "        \"\"\"\n",
    "        # Apply first convolution, ReLU activation, and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Apply second convolution, ReLU activation, and max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten the feature maps for the fully connected layers\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        # Apply the first fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Apply dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "        # Apply the final output layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_and_evaluate(learning_rate, device, n_epochs=25, batch_size=128, p_bar=None):\n",
    "    \"\"\"Orchestrates the training and evaluation of a model for a given configuration.\n",
    "\n",
    "    This function handles the end-to-end workflow: setting a random seed,\n",
    "    initializing the model, optimizer, loss function, and dataloaders, and then\n",
    "    running the main training loop.\n",
    "\n",
    "    Args:\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "        device: The device (e.g., 'cuda' or 'cpu') for training and evaluation.\n",
    "        n_epochs (int, optional): The number of training epochs. Defaults to 25.\n",
    "        batch_size (int, optional): The batch size for dataloaders. Defaults to 128.\n",
    "        p_bar (optional): An existing progress bar handler. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the training and validation history\n",
    "              (loss and accuracy).\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    helper_utils.set_seed(42)\n",
    "\n",
    "    # Initialize the model and move it to the specified device\n",
    "    model = SimpleCNN().to(device)\n",
    "\n",
    "    # Define the optimizer with the specified learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Prepare the training and validation dataloaders\n",
    "    train_loader, val_loader = helper_utils.get_dataset_dataloaders(\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Call the main training loop to train the model and get the history\n",
    "    history = helper_utils.train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_fcn=loss_fn,\n",
    "        device=device,\n",
    "        n_epochs=n_epochs,\n",
    "        p_bar=p_bar\n",
    "    )\n",
    "\n",
    "    # Return the collected training history\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a1d2c",
   "metadata": {},
   "source": [
    "Now the model is trained for several epochs for each of the specified learning rates.\n",
    "The results are stored in `training_curves`, and the curves (train loss and validation accuracy) are plotted for each learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48ca10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Different learning rates to be analyzed\n",
    "learning_rates = [0.0002, 0.001, 0.005] # Small, medium, and large learning rates\n",
    "\n",
    "training_curves = []\n",
    "n_epochs = 25\n",
    "batch_size = 128\n",
    "\n",
    "p_bar = helper_utils.get_p_bar(n_epochs)\n",
    "\n",
    "# Get the total number of learning rates to check against the index\n",
    "num_learning_rates = len(learning_rates)\n",
    "\n",
    "# Train and evaluate the model for each learning rate\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    print(f\"\\nTraining with learning rate: {lr}\\n\")\n",
    "    history = train_and_evaluate(learning_rate=lr, n_epochs=n_epochs, batch_size=batch_size, device=device, p_bar=p_bar)\n",
    "    training_curves.append(history)\n",
    "    # Only reset the progress bar if it's NOT the last iteration\n",
    "    if i < num_learning_rates - 1:\n",
    "        p_bar.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'orange', 'red']\n",
    "labels = ['Low LR', 'Medium LR', 'High LR']\n",
    "\n",
    "helper_utils.plot_learning_curves(colors, labels, training_curves)\n",
    "\n",
    "# Each color corresponds to a different learning rate: blue for low, orange for medium, and red for high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224bc3f",
   "metadata": {},
   "source": [
    "### Learning Rate Analysis\n",
    "\n",
    "Each color represents a different learning rate: blue for low, orange for medium, and red for high.\n",
    "\n",
    "- **Low Learning Rate:**\n",
    "The validation accuracy improves gradually and may require a significant amount of time (many epochs) to reach its peak.\n",
    "\n",
    "- **Medium Learning Rate:**\n",
    "The validation accuracy increases quickly, and by the end of the 25 epochs, it is outperforming the other two models.\n",
    "\n",
    "- **High Learning Rate:**\n",
    "Initially, validation accuracy is the highest during the first few epochs but eventually levels off, indicating instability where the model struggles to improve with such a high learning rate.\n",
    "\n",
    "Among these options, the medium learning rate strikes the best balance between learning speed and generalization. Could you achieve a better model by combining these learning rates—starting with a high rate for the first few epochs, then transitioning to medium, and finally to low?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc8697",
   "metadata": {},
   "source": [
    "## Schedulers\n",
    "In the previous section, you explored how the choice of learning rate impacts model performance. A high learning rate can lead to instability, while a low one might slow down training or yield suboptimal outcomes. Learning rate schedulers offer a solution by automatically adjusting the learning rate throughout training, initially setting it high for rapid learning and then reducing it to enhance convergence and generalization.\n",
    "\n",
    "In this lab, you'll investigate three different schedulers. Let's start with the `StepLR` scheduler, which lowers the learning rate by a specified factor at regular intervals—specifically, after a set number of epochs. You'll repeat the training process from before, but this time you'll employ a scheduler that decreases the learning rate by a factor of 0.2 every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3269cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.set_seed(42)\n",
    "\n",
    "# Initialize the model, optimizer, loss function, and dataloaders\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005) # start with a high learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36aa2a0",
   "metadata": {},
   "source": [
    "Schedulers are built into the PyTorch `torch.optim.lr_scheduler` module and can be easily integrated into your training loop.\n",
    "You typically initialize a scheduler with the optimizer and parameters that define how the learning rate changes over time.\n",
    "\n",
    "The `StepLR` scheduler, for example:\n",
    "  - Takes the optimizer as input.\n",
    "  - Requires `step_size`, the number of epochs before reducing the learning rate.\n",
    "  - Uses a `gamma` factor, the factor by which the learning rate is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaca0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2) # reduce the learning rate by 20% it's prior value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e280a852",
   "metadata": {},
   "source": [
    "The training process is similar to the previous one, but the learning rate is adjusted by the scheduler after each epoch using the `scheduler.step()` method.\n",
    "The current learning rate can be accessed using `scheduler.get_last_lr()`.\n",
    "\n",
    "The model is trained for several epochs with the `StepLR` scheduler, and the training curves are plotted to compare the results with the previous experiments for the medium learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027afce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader, val_loader = helper_utils.get_dataset_dataloaders(batch_size=batch_size)\n",
    "\n",
    "history_LR = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"lr\": [],\n",
    "}\n",
    "\n",
    "\n",
    "pbar = helper_utils.NestedProgressBar(\n",
    "    total_epochs=n_epochs,\n",
    "    total_batches=len(train_loader),\n",
    "    epoch_message_freq=5,\n",
    "    mode=\"train\",\n",
    ")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    pbar.update_epoch(epoch+1)\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    train_loss, train_acc = helper_utils.train_epoch(model, train_loader, optimizer, loss_fn, device, pbar)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_acc = helper_utils.evaluate_epoch(model, val_loader, loss_fn, device)\n",
    "\n",
    "    # Get the current learning rate BEFORE stepping the scheduler.\n",
    "    # This captures the LR that was just used for the training epoch above.\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # Step the scheduler (updates the LR for the NEXT epoch)\n",
    "    scheduler.step()\n",
    "    \n",
    "    pbar.maybe_log_epoch(epoch=epoch+1, message=f\"At epoch {epoch+1}: Training loss: {train_loss:.4f}, Training accuracy: {train_acc:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "    pbar.maybe_log_epoch(epoch=epoch+1, message=f\"At epoch {epoch+1}: Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    history_LR[\"train_loss\"].append(train_loss)\n",
    "    history_LR[\"train_acc\"].append(train_acc)\n",
    "    history_LR[\"val_loss\"].append(val_loss)\n",
    "    history_LR[\"val_acc\"].append(val_acc)\n",
    "    history_LR[\"lr\"].append(current_lr)\n",
    "\n",
    "pbar.close('Training complete with StepLR scheduler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8425aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "history_constant = training_curves[idx]\n",
    "\n",
    "colors = ['orange', 'green']\n",
    "labels = ['Medium LR', 'Step LR']\n",
    "histories = [history_constant, history_LR]\n",
    "\n",
    "helper_utils.plot_learning_curves(colors, labels, histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c808ac",
   "metadata": {},
   "source": [
    "### Analysis of Medium LR and StepLR Scheduler\n",
    "\n",
    "The plots show a comparison between training with a constant learning rate (orange) and using a `StepLR` scheduler (green). With StepLR, the training loss is significantly lower than with a fixed learning rate. You can also notice a few dips at the epoch 10 and 20 when the learning rate changes.  Although the validation accuracy starts off stronger, both models eventually converge to similar performance levels at the last epoch. Now, let's examine two more schedulers to compare their effectiveness.\n",
    "\n",
    "### Other Schedulers\n",
    "\n",
    "There are many other schedulers available in PyTorch, such as `CosineAnnealingLR`, and `ReduceLROnPlateau`, each with its own strategy for adjusting the learning rate.\n",
    "You can explore these options in the [PyTorch documentation](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate).\n",
    "\n",
    "Below `train_and_evaluate_with_scheduler` function extends the previous `train_and_evaluate` function to include a learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_with_scheduler(model, optimizer, scheduler, device, n_epochs=25, batch_size=128):\n",
    "    \"\"\"Trains and evaluates a model using a learning rate scheduler.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to be trained.\n",
    "        optimizer: The optimization algorithm.\n",
    "        scheduler: The learning rate scheduler.\n",
    "        device: The computing device ('cuda' or 'cpu') to run the training on.\n",
    "        n_epochs: The total number of training epochs.\n",
    "        batch_size: The number of samples per batch in the data loaders.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the training and validation history\n",
    "        (loss, accuracy, and learning rate) for each epoch.\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    helper_utils.set_seed(10)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Prepare the training and validation data loaders\n",
    "    train_loader, val_loader = helper_utils.get_dataset_dataloaders(\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Initialize a dictionary to store training and validation history\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        'lr': [],\n",
    "    }\n",
    "\n",
    "    # Initialize the progress bar for monitoring training\n",
    "    pbar = helper_utils.NestedProgressBar(\n",
    "        total_epochs=n_epochs,\n",
    "        total_batches=len(train_loader),\n",
    "        epoch_message_freq=5,\n",
    "        mode=\"train\",\n",
    "    )\n",
    "\n",
    "    # Loop through the specified number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Update the progress bar for the current epoch\n",
    "        pbar.update_epoch(epoch+1)\n",
    "\n",
    "        # Train the model for one epoch\n",
    "        train_loss, train_acc = helper_utils.train_epoch(model, train_loader, optimizer, loss_fn, device, pbar)\n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss, val_acc = helper_utils.evaluate_epoch(model, val_loader, loss_fn, device)\n",
    "        \n",
    "        # Retrieve the current learning rate from the scheduler\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        # Update the learning rate based on the scheduler type\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            # For schedulers that monitor a metric, pass the metric to the step function\n",
    "            scheduler.step(val_acc)\n",
    "        else:\n",
    "            # For other schedulers, call the step function without arguments\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Log the training metrics for the current epoch, including the learning rate\n",
    "        pbar.maybe_log_epoch(epoch=epoch+1, message=f\"At epoch {epoch+1}: Training loss: {train_loss:.4f}, Training accuracy: {train_acc:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "        # Log the validation metrics for the current epoch, including the learning rate\n",
    "        pbar.maybe_log_epoch(epoch=epoch+1, message=f\"At epoch {epoch+1}: Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        # Append the metrics for the current epoch to the history dictionary\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "\n",
    "    # Close the progress bar upon completion of training\n",
    "    pbar.close('Training complete!')\n",
    "    # Return the collected training and validation history\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae0651",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In the following cells, you will run experiments using both `CosineAnnealingLR` and `ReduceLROnPlateau` schedulers. The training curves for each scheduler will be generated and compared to visualize their effects on model performance.\n",
    "\n",
    "- **CosineAnnealingLR:**  \n",
    "    This scheduler adjusts the learning rate following a cosine curve, gradually reducing it from the initial value to a minimum over a specified number of epochs.  \n",
    "    Parameters used in this notebook are:  \n",
    "    - `optimizer`: The optimizer to schedule.  \n",
    "    - `T_max`: Number of epochs for one cycle of cosine annealing.\n",
    "    - `eta_min`: Minimum learning rate.\n",
    ">\n",
    "- **ReduceLROnPlateau:**  \n",
    "    This scheduler monitors a metric (such as validation loss) and reduces the learning rate by a factor if no improvement is seen for a set number of epochs.  \n",
    "    Parameters used in this notebook:  \n",
    "    - `optimizer`: The optimizer to schedule.  \n",
    "    - `mode`: Whether to look for a decrease (`min`) or increase (`max`) in the monitored metric.  \n",
    "    - `factor`: Factor by which the learning rate will be reduced.  \n",
    "    - `patience`: Number of epochs with no improvement before reducing the learning rate.\n",
    "\n",
    "For more details and additional parameters, see the [PyTorch documentation](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CosineAnnealingLR\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min = 0.0002)\n",
    "\n",
    "history_cosine = train_and_evaluate_with_scheduler(\n",
    "    model, optimizer, scheduler_cosine, device, n_epochs=n_epochs, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24072fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReduceLROnPlateau\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=3)\n",
    "\n",
    "history_plateau = train_and_evaluate_with_scheduler(\n",
    "    model, optimizer, scheduler_plateau, device, n_epochs=n_epochs, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2044e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Medium LR', 'StepLR', 'CosineAnnealingLR', 'ReducedLRonPlateau']\n",
    "colors = ['orange', 'green', 'blue', 'purple']\n",
    "\n",
    "training_curves_new = [history_constant, history_LR, history_cosine, history_plateau]\n",
    "\n",
    "helper_utils.plot_learning_curves(colors, labels, training_curves_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f61b3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Upon analyzing the validation accuracy, you see that both the `ReduceLROnPlateau` and `CosineAnnealingLR` schedulers yield improved results when you observe the last epoch. Keep in mind that each scheduler has tunable hyperparameters as well. \n",
    "\n",
    "Below, you illustrate how the learning rate changes over the epochs for each scheduler. The `CosineAnnealingLR` scheduler reduces the learning rate according to a cosine decay pattern, while `ReduceLROnPlateau` adjusts it dynamically based on validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_learning_rates_curves(training_curves_new, colors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1cc8d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In summary:\n",
    "\n",
    "- `StepLR` is effective for reducing the learning rate at fixed intervals in a stepwise manner.\n",
    "\n",
    "- `CosineAnnealingLR` offers a smooth decay without sudden changes, which is beneficial for fine-tuning the model as training concludes. It's particularly advantageous for long training sessions where gradual adjustments can enhance convergence.\n",
    "\n",
    "- `ReduceLROnPlateau` is useful when validation performance plateaus, as it adjusts the learning rate based on validation metrics, potentially improving generalization. It is responsive to the model's performance, allowing for dynamic adjustments based on training dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc8d66",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations on completing this in-depth exploration of PyTorch's learning rate schedulers! In this notebook, you examined how dynamically adjusting the learning rate can enhance training efficiency, speed up convergence, and improve generalization when compared to a constant learning rate. By utilizing schedulers and visualizing their effects on training and validation metrics, you observed how they contribute to improved model performance. Additionally, you gained valuable experience in tuning scheduler hyperparameters to suit various training scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
