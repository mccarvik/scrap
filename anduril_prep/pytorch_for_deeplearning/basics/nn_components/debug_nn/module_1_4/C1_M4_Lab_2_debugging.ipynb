{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf5dcf7",
   "metadata": {},
   "source": [
    "# Model Debugging, Inspection, and Modularization\n",
    "\n",
    "So far, you've focused on building and training models. \n",
    "But in the real world, your first attempt at a model rarely works perfectly. \n",
    "You'll often encounter cryptic error messages about mismatched tensor shapes, or worse, your model will run without errors but fail to produce meaningful results. \n",
    "\n",
    "This is where **debugging, inspection, and modularization** become essential skills. \n",
    "In this lab, you'll step into the role of a model investigator. \n",
    "You'll start with a broken Convolutional Neural Network (CNN) and use systematic debugging techniques to find and fix the bug. Then, you'll learn how to refactor your code for clarity and reuse, and finally, you'll dissect a complex, pre-trained model to understand its inner workings.\n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "* **Debug** a broken CNN by inserting print statements into the `forward` pass to identify and correct a critical tensor shape mismatch.\n",
    "* **Refactor** the corrected model using `nn.Sequential` to create a cleaner, more modular, and less error-prone architecture.\n",
    "* **Inspect** the activation statistics of your model to perform a sanity check for issues like exploding or vanishing gradients.\n",
    "* **Explore** the architecture of a complex, pre-existing model (`SqueezeNet`) to count its layers and analyze its parameter distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197b69c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457aa723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129c668",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "To debug and inspect a model effectively, you'll first need a dataset to work with.\n",
    "The goal of this lab is to practice an end-to-end debugging and inspection workflow, so you'll use a simple dataset that lets you focus on the model architecture rather than complex data preprocessing.\n",
    "For this purpose, you'll use the Fashion MNIST dataset, which consists of grayscale images of clothing items and serves as a straightforward benchmark for image classification tasks.\n",
    "You’ll begin by loading the dataset using PyTorch’s torchvision library, and then create a DataLoader to efficiently handle the data in batches during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = helper_utils.get_dataset()\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "dataset.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b101dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch, label_batch = next(iter(dataloader))\n",
    "print(\"Batch shape:\", img_batch.shape)  # Should be [batch_size, 1, 28, 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74643a59",
   "metadata": {},
   "source": [
    "## Debugging through forward pass\n",
    "\n",
    "When starting to work with a new model, it is common to encounter errors. \n",
    "These errors can be due to various reasons, such as incorrect tensor shapes, incompatible operations, or unexpected values.\n",
    "Sometimes, the model may run without errors but produce incorrect outputs.\n",
    "\n",
    "In this section, you will explore how to debug a PyTorch model by examining its forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573827f6",
   "metadata": {},
   "source": [
    "### A first exploration of the model\n",
    "\n",
    "It is now time to explore the model.\n",
    "This model is a simple network with:\n",
    "* a convolutional block: consisting of a convolutional layer, a ReLU activation function, and a max pooling layer,\n",
    "* a fully connected block: consisting of a linear layer, a ReLU activation function, and a final linear layer that outputs the class scores.\n",
    "\n",
    "You will first instantiate the model and try to run a forward pass with a batch from the dataloader.\n",
    "To get a cleaner output in case of errors, you will use `try/except` to catch any exceptions that may arise during the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de59fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolutional Block\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully Connected Block\n",
    "        # For Fashion MNIST: input images are 28x28,\n",
    "        # after conv+pool: 32x14x14\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)\n",
    "        self.relu_fc = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 classes for Fashion MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv(x)))\n",
    "        x = self.relu_fc(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = SimpleCNN()\n",
    "\n",
    "try:\n",
    "    output = simple_cnn(img_batch)  \n",
    "except Exception as e:\n",
    "    print(f\"\\033[91mError during forward pass: {e}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef02ac3c",
   "metadata": {},
   "source": [
    "Indeed, the model as provided contains some errors that require debugging.\n",
    "The message provided by PyTorch when an error occurs can sometimes be cryptic.\n",
    "It describes that two matrices (`mat1` and `mat2`) cannot be multiplied and provides their shapes.\n",
    "This indicates that there is a mismatch in the dimensions of the tensors being multiplied, which is a common issue in neural network implementations.\n",
    "\n",
    "However, the error message does not specify **why and where** in the model the error occurs.\n",
    "That is when the `forward` method of the model comes into play.\n",
    "The dynamic graph nature of PyTorch allows you to insert print statements or use debugging tools to inspect the values and shapes of tensors at various points in the `forward` method.\n",
    "\n",
    "You will define a new class that inherits from the original model and overrides the `forward` method to include print statements that display the shape of the tensor after each layer.\n",
    "A first try might be to explicitly separate the layers in the `forward` method and, for each layer:\n",
    "* print the shape of the tensor before the layer (input shape),\n",
    "* print the shape of some *parameters of the layer* (e.g., weights and biases),\n",
    "* print the shape of the *activation* tensor after the layer (output shape), which will be the input for the next layer.\n",
    "\n",
    "You can now run the forward pass again and observe the printed shapes to identify where the mismatch occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNDebug(SimpleCNN):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # The super().__init__() call above properly initializes all layers from SimpleCNN\n",
    "        # No need to redefine the layers here\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        print(\n",
    "            \" (Layer components) Conv layer parameters (weights, biases):\",\n",
    "            self.conv.weight.shape,\n",
    "            self.conv.bias.shape,\n",
    "        )\n",
    "        x_conv = self.relu(self.conv(x))\n",
    "\n",
    "        print(\"===\")\n",
    "\n",
    "        print(\"(Activation) After convolution and ReLU:\", x_conv.shape)\n",
    "        x_pool = self.pool(x_conv)\n",
    "        print(\"(Activation) After pooling:\", x_pool.shape)\n",
    "\n",
    "        print(\n",
    "            \"(Layer components) Linear layer fc1 parameters (weights, biases):\",\n",
    "            self.fc1.weight.shape,\n",
    "            self.fc1.bias.shape,\n",
    "        )\n",
    "\n",
    "        x_fc1 = self.relu_fc(self.fc1(x_pool))\n",
    "\n",
    "        print(\"===\")\n",
    "\n",
    "        print(\"(Activation) After fc1 and ReLU:\", x_fc1.shape)\n",
    "\n",
    "        print(\n",
    "            \"(Layer components) Linear layer fc2 parameters (weights, biases):\",\n",
    "            self.fc2.weight.shape,\n",
    "            self.fc2.bias.shape,\n",
    "        )\n",
    "        x = self.fc2(x_fc1)\n",
    "\n",
    "        print(\"===\")\n",
    "\n",
    "        print(\"(Activation) After fc2 (output):\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223eec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn_debug = SimpleCNNDebug()\n",
    "\n",
    "try:\n",
    "    output_debug = simple_cnn_debug(img_batch)  \n",
    "except Exception as e:\n",
    "    print(f\"\\033[91mError during forward pass in debug model: {e}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ef090",
   "metadata": {},
   "source": [
    "This is already a cleaner output. \n",
    "You can already see that all the layers of the convolutional block are working fine, and the shapes are as expected (`batch_size=64` and `out_channels=32`).\n",
    "\n",
    "**The error occurs in the fully connected block**, specifically at the first linear layer:\n",
    "`x_pool` has shape `[64, 32, 14, 14]`, but the linear layer expects an input of shape `[64, 2048]` (its weight matrix has shape `[128, 6272]`).\n",
    "\n",
    "As the linear layer `fc1` expects a 2D input of shape `[batch_size, input_features]`, the `x_pool` is flattened to a 2D tensor with shape `[64*32*14, 14]` before being passed to `fc1`.\n",
    "This is not the intended shape, and it leads to the dimension mismatch error.\n",
    "\n",
    "Once you have identified the issue, you can fix it by adding a flattening operation before the first linear layer in the `forward` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac0adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNFixed(SimpleCNN):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input shape:\", x.shape)\n",
    "        print(\n",
    "            \" (Neuron components) Conv layer parameters (weights, biases):\",\n",
    "            self.conv.weight.shape,\n",
    "            self.conv.bias.shape,\n",
    "        )\n",
    "        x_conv = self.relu(self.conv(x))\n",
    "\n",
    "        print(\"===\")\n",
    "\n",
    "        print(\"(Activation) After convolution and ReLU:\", x_conv.shape)\n",
    "        x_pool = self.pool(x_conv)\n",
    "        print(\"(Activation) After pooling:\", x_pool.shape)\n",
    "\n",
    "        x_flattened = torch.flatten(\n",
    "            x_pool, start_dim=1\n",
    "        )  # Flatten all dimensions except batch\n",
    "        print(\"(Activation) After flattening:\", x_flattened.shape)\n",
    "\n",
    "        print(\n",
    "            \"(Neuron components) Linear layer fc1 parameters (weights, biases):\",\n",
    "            self.fc1.weight.shape,\n",
    "            self.fc1.bias.shape,\n",
    "        )\n",
    "\n",
    "        x_fc1 = self.relu_fc(self.fc1(x_flattened))\n",
    "\n",
    "        print(\"===\")\n",
    "\n",
    "        print(\"(Activation) After fc1 and ReLU:\", x_fc1.shape)\n",
    "\n",
    "        print(\n",
    "            \"(Neuron components) Linear layer fc2 parameters (weights, biases):\",\n",
    "            self.fc2.weight.shape,\n",
    "            self.fc2.bias.shape,\n",
    "        )\n",
    "        x = self.fc2(x_fc1)\n",
    "\n",
    "        print(\"===\")\n",
    "\n",
    "        print(\"(Activation) After fc2 (output):\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfbd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed version\n",
    "simple_cnn_fixed = SimpleCNNFixed()\n",
    "\n",
    "output = simple_cnn_fixed(img_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7a8b9",
   "metadata": {},
   "source": [
    "The issue is now fixed, and the model runs without errors! You can see that the shapes of the tensors are as expected after each layer, and the final output has the correct shape of `[64, 10]`, corresponding to the batch size and the number of classes.\n",
    "\n",
    "Once the model is running without errors, you can jump the next section to refactor the model using `nn.Sequential` for a cleaner and more modular implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300a1ca",
   "metadata": {},
   "source": [
    "## `nn.Sequential` for Modularization\n",
    "\n",
    "The model is now working correctly, but the `forward` method is quite verbose and repetitive.\n",
    "To make the code cleaner and more modular, you can use `nn.Sequential` to define the convolutional and fully connected blocks.\n",
    "\n",
    "In this way you gain several advantages:\n",
    "* **Modularity**: Each block is defined as a separate module, making it easier to understand and modify.\n",
    "* **Reusability**: You can easily reuse the blocks in other models or experiments.\n",
    "* **Cleaner Code**: The `forward` method becomes much simpler, as it only needs to call the blocks sequentially.\n",
    "* **Less Error-Prone**: By defining the blocks in one place, you reduce the chances of making mistakes when implementing the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a442781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolutional Block\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Fully Connected Block\n",
    "        # For Fashion MNIST: input images are 28x28,\n",
    "        # after conv+pool: 32x14x14\n",
    "        flattened_size = 32 * 14 * 14\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),  # 10 classes for Fashion MNIST\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten all dimensions except batch\n",
    "        x = self.fc_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45aa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn_seq = SimpleCNN2Seq()\n",
    "output = simple_cnn_seq(img_batch)\n",
    "\n",
    "print(\"Output shape from sequential model:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac6ddb",
   "metadata": {},
   "source": [
    "### Statistical Inspection of the Initialization\n",
    "\n",
    "A common check when inspecting a model is to look at the statistics of some activations to ensure that they are within a reasonable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN2SeqDebug(SimpleCNN2Seq):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # The super().__init__() call above properly initializes all layers from SimpleCNN2Seq\n",
    "        # No need to redefine the layers here\n",
    "\n",
    "    def get_statistics(self, activation):\n",
    "        mean = activation.mean().item()\n",
    "        std = activation.std().item()\n",
    "        min_val = activation.min().item()\n",
    "        max_val = activation.max().item()\n",
    "\n",
    "        print(f\" Mean: {mean}\")\n",
    "        print(f\" Std: {std}\")\n",
    "        print(f\" Min: {min_val}\")\n",
    "        print(f\" Max: {max_val}\")\n",
    "        return mean, std, min_val, max_val\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.conv_block(x)\n",
    "        x = torch.flatten(features, start_dim=1)  # Flatten all dimensions except batch\n",
    "\n",
    "        print(\"After conv_block, the activation statistics are:\")\n",
    "        self.get_statistics(features)\n",
    "\n",
    "        x = self.fc_block(x)\n",
    "        print(\"After fc_block, the activation statistics are:\")\n",
    "        self.get_statistics(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn_seq_debug = SimpleCNN2SeqDebug()\n",
    "\n",
    "for idx, (img_batch, _) in enumerate(dataloader):\n",
    "    if idx < 5:\n",
    "        print(f\"=== Batch {idx} ===\")\n",
    "        output_debug = simple_cnn_seq_debug(img_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b712b09",
   "metadata": {},
   "source": [
    "This is a sanity check to ensure that the model is initialized correctly and that the activations are not exploding or vanishing.\n",
    "*Those issues can lead to poor training performance or convergence problems.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e4812",
   "metadata": {},
   "source": [
    "## Model Inspection\n",
    "\n",
    "With the previous model working correctly, you will now inspect a pre-existing complex model from `torchvision.models`, such as `SqueezeNet`.\n",
    "\n",
    "In this section you will make use of the inspection utilities provided by PyTorch to explore the model's architecture, layers, and parameters.\n",
    "These inspection techniques are foundational for effective debugging and for making informed modifications to your neural network designs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8498a45e",
   "metadata": {},
   "source": [
    "### Architecture Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9fd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SqueezeNet model\n",
    "complex_model = SqueezeNet()\n",
    "\n",
    "print(complex_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ff125",
   "metadata": {},
   "source": [
    "For complex models, printing the entire model architecture can be overwhelming.\n",
    "Instead, you can make use of `named_children()` and `children()` to iterate through the top-level blocks of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271910ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the main blocks\n",
    "for name, block in complex_model.named_children():\n",
    "    print(f\"Block {name} has a total of {len(list(block.children()))} layers:\")\n",
    "    \n",
    "    # List all children layers in the block\n",
    "    for idx, layer in enumerate(block.children()):\n",
    "        # Check if the layer is terminal (no children) or not\n",
    "        if len(list(layer.children())) == 0:\n",
    "            print(f\"\\t {idx} - Layer {layer}\")\n",
    "        # If the layer has children, it's a sub-block, then print only the number of children and its name\n",
    "        else:\n",
    "            layer_name = layer._get_name()  # More user-friendly name\n",
    "            print(f\"\\t {idx} - Sub-block {layer_name} with {len(list(layer.children()))} layers\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac451a",
   "metadata": {},
   "source": [
    "This provides a cleaner overview of the model's structure, allowing you to focus on the main components without getting lost in the details of every single layer.\n",
    "You will now zoom into one of the `Fire` modules to see its internal structure.\n",
    "\n",
    "For that you can use `modules()` to iterate through all the layers and sub-modules of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_fire_module = complex_model.features[3]\n",
    "\n",
    "for idx, module in enumerate(first_fire_module.modules()):\n",
    "    # Avoid printing the top-level module itself\n",
    "    if idx > 0 :\n",
    "        print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a189bb",
   "metadata": {},
   "source": [
    "Now the model's architecture is neatly printed, showing the main components and their configurations.\n",
    "You can now do some specific inspections such as counting the number of specific layer types or calculating the total number of parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52912d",
   "metadata": {},
   "source": [
    "### Detail Inspection\n",
    "\n",
    "You will now count how many `Conv2d` layers are in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_layer = nn.Conv2d\n",
    "\n",
    "selected_layers = [layer for layer in complex_model.modules() if isinstance(layer, type_layer)]\n",
    "\n",
    "print(f\"Number of {type_layer.__name__} layers: {len(selected_layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a2914",
   "metadata": {},
   "source": [
    "You will now count the total number of parameters in the model.\n",
    "This gives you an idea of the model's complexity and capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87da032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of parameters in the model\n",
    "total_params = sum(p.numel() for p in complex_model.parameters())\n",
    "print(f\"Total number of parameters in the model: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c5b1b",
   "metadata": {},
   "source": [
    "Now, you can take this further by inspecting the parameters of each terminal layer (layers without children) in the model.\n",
    "For each terminal layer, you will print its name and the total number of parameters it contains.\n",
    "This helps to identify which layers contribute most to the model's parameter count and can be useful for model optimization, pruning, or understanding where the model's capacity lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_params = {}\n",
    "\n",
    "# For each terminal layer print its number of parameters\n",
    "for layer in complex_model.named_modules():\n",
    "    n_children = len(list(layer[1].children()))\n",
    "    if n_children == 0:  # Terminal layer\n",
    "        layer_name = layer[0]\n",
    "        n_parameters = sum(p.numel() for p in layer[1].parameters())\n",
    "        counting_params[layer_name] = n_parameters\n",
    "        print(f\"Layer {layer_name} has {n_parameters} parameters\")\n",
    "\n",
    "# Plotting the distribution of parameters per layer\n",
    "helper_utils.plot_counting(counting_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda7eac",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "You have now successfully debugged, refactored, and inspected PyTorch models. \n",
    "In this lab, you saw firsthand that a model's `forward` pass is not a black box and that by strategically adding print statements, you can diagnose and solve common but frustrating errors like shape mismatches.\n",
    "\n",
    "You have moved beyond simply writing model code and can now make it more robust and readable by grouping layers into logical blocks with **`nn.Sequential`**. \n",
    "This practice of modularization makes your architectures easier to understand, reuse, and adapt. \n",
    "You also learned how to perform essential sanity checks by inspecting activation statistics and how to systematically explore any PyTorch model, no matter how complex, using inspection utilities like `.modules()` and `.named_children()`.\n",
    "\n",
    "With these fundamental skills of debugging and inspection, you are well-prepared for more advanced challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
