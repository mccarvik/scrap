{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f42444-02f9-4590-a7df-ac139cb214d8",
   "metadata": {},
   "source": [
    "# Building a CNN for Nature Classification\n",
    "\n",
    "Welcome! The butterfly house next door has approached the botanical garden with a new idea: an app that can classify not just flowers, but also insects and small animals. Your task is to design and build the model that will make this possible.\n",
    "\n",
    "This problem is more complex than what you’ve tackled before. The linear layers you used previously won’t be enough to capture the rich visual patterns in these diverse images. To meet this new challenge, you’ll build a **Convolutional Neural Network (CNN)**, a model designed to recognize shapes, textures, and features in visual data.\n",
    "\n",
    "In this lab, you’ll go through the end-to-end process of building a CNN for this classification task. You’ll not only implement the architecture but also follow an iterative workflow—starting with a smaller prototype before scaling up—and learn how to diagnose common training issues.\n",
    "\n",
    "You will:\n",
    "\n",
    "* **Prepare a Diverse Dataset**: Load and transform a specialized subset of images for your multi-class nature classifier.\n",
    "\n",
    "* **Build a CNN Architecture**: Define a complete CNN from scratch, combining convolutional, pooling, and fully connected layers to create a powerful feature extractor.\n",
    "\n",
    "* **Train a Prototype Model**: Follow a realistic workflow by first training your model on a smaller, 9-class subset to build a working prototype and establish a performance baseline.\n",
    "\n",
    "* **Scale Up and Diagnose Challenges**: Train the full model on all 15 classes and analyze the results to identify common machine learning challenges like overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f583f9a-3bc9-4ef1-b846-6803314e4dd1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33a321-ef93-4897-8b99-36f771c1ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1e750-6b47-4ce2-be85-b518be0e99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c09430-a1cf-4fb5-ac06-8fd94af39a19",
   "metadata": {},
   "source": [
    "## Preparing the Nature Dataset\n",
    "\n",
    "For this lab, you'll work with a collection of images taken from the well-known [CIFAR-100 dataset](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.CIFAR100.html). This dataset is a fantastic resource for computer vision tasks, containing thousands of small, **32x32 color images** that are perfect for training a CNN. It’s a diverse collection, which is exactly what you need for the expanded nature classifier app.\n",
    "\n",
    "While CIFAR-100 has 100 different classes, you won't need all of them. To meet the new requirements for your app, you'll focus on a curated selection of **15 classes** that fit the theme of a nature classifier. This selection will include flowers, insects and mammals. Specifically, you'll be working with:\n",
    "\n",
    "* **Flowers**: 'orchid', 'poppy', 'rose', 'sunflower', 'tulip'\n",
    "\n",
    "* **Mammals**: 'fox', 'porcupine', 'possum', 'raccoon', 'skunk'\n",
    "\n",
    "* **Insects**: 'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'\n",
    "\n",
    "\n",
    "### Image Transformations\n",
    "\n",
    "Before you load the dataset, first you need to define the transformation pipelines for it. Since all images in the dataset are already a standard **32x32** size, you don't need to add a resizing step. Your training pipeline will include data augmentation, while both pipelines will convert images to **tensors** and **normalize** them using the standard mean and standard deviation for the CIFAR-100 dataset.\n",
    "\n",
    "* Define the specific mean and standard deviation for the CIFAR-100 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5785e2-916d-4fbf-b753-85416867acaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
    "cifar100_std = (0.2675, 0.2565, 0.2761)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e2785-647d-4bcc-a493-bd63e523bf43",
   "metadata": {},
   "source": [
    "* Define two separate pipelines using `transforms.Compose`.\n",
    "    * One for the training set that includes data augmentation and another for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca34921-7ebc-42c6-b153-936ae25e140a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training set transformation pipeline\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar100_mean, cifar100_std)\n",
    "])\n",
    "\n",
    "# Validation set transformation pipeline\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar100_mean, cifar100_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778beb43-2e5b-467c-8c3f-f78b784f0bea",
   "metadata": {},
   "source": [
    "### Preparing the Data Pipeline\n",
    "\n",
    "With your transformations ready, it's time to load the data. To quickly show the butterfly house next door a working prototype, it's a smart strategy to start with a smaller, more manageable dataset. This allows you to test your entire pipeline and build a baseline model without the long wait times required for the full dataset.\n",
    "\n",
    "Therefore, instead of using all 15 classes at once, you'll begin with a balanced subset of **9 classes** (**3 from each category**). This iterative approach is a common and efficient practice in real-world machine learning.\n",
    "\n",
    "For this initial prototype, you'll use the following classes:\n",
    "\n",
    "* **Flowers**: 'orchid', 'poppy', 'sunflower'\n",
    "\n",
    "* **Mammals**: 'fox', 'raccoon', 'skunk'\n",
    "\n",
    "* **Insects**: 'butterfly', 'caterpillar', 'cockroach'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596fb0ff-ef2e-4df3-9ff2-9eb08b477037",
   "metadata": {},
   "source": [
    "* Create a Python list containing the names of the 9 classes you'll use for the initial prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69421f3-9f65-4fb3-8369-19b01b25f3b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subset_target_classes = [\n",
    "    # Flowers\n",
    "    'orchid', 'poppy', 'sunflower',\n",
    "    # Mammals\n",
    "    'fox', 'raccoon', 'skunk',\n",
    "    # Insects\n",
    "    'butterfly', 'caterpillar', 'cockroach'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31928c9-056f-40fe-b593-d3e43bcb9e0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Use the `load_cifar100_subset` helper function, passing in your `subset_target_classes` list and both transformation pipelines.\n",
    "* This function handles the entire loading process: it downloads the full CIFAR-100 dataset, applies your specified transformations, and then filters the result to include only the **9 classes** you selected.\n",
    "* It returns the final training and validation dataset objects, ready for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d805d7-c101-4fa7-82c0-3baa99415f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the helper function to prepare the datasets\n",
    "train_dataset_proto, val_dataset_proto = helper_utils.load_cifar100_subset(subset_target_classes, train_transform, val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e20de-b8e9-4dda-b1d8-f20919a48fc4",
   "metadata": {},
   "source": [
    "* With your `Dataset` objects ready, the final step in the data pipeline is to create `DataLoaders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0bbe4-86ea-497c-8de3-e60628571a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the number of samples to be processed in each batch\n",
    "batch_size = 64\n",
    "\n",
    "# Create a data loader for the training set, with shuffling enabled\n",
    "train_loader_proto = DataLoader(train_dataset_proto, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a data loader for the validation set, without shuffling\n",
    "val_loader_proto = DataLoader(val_dataset_proto, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad9296-f995-430d-a2c0-4235644547d1",
   "metadata": {},
   "source": [
    "### Visualizing the Training Images\n",
    "\n",
    "With your data pipeline complete, it's always a good idea to look at a few examples from your training set. This helps confirm that your data has been loaded and processed correctly. The following helper function will display a random sample of your training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fafe015-9d6c-4e4d-ba82-5879d9ff6c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize a 3x3 grid of random training images\n",
    "helper_utils.visualise_images(train_dataset_proto, grid=(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaba373-fafb-415d-bf39-c323d6c05e91",
   "metadata": {},
   "source": [
    "## Building the CNN Architecture\n",
    "\n",
    "With your data ready, it's time to build the core of your nature classifier. For a complex task like identifying different species in images, the linear layers you've used before aren't enough, as they look at pixels individually without understanding their spatial relationships.\n",
    "\n",
    "You'll now build a **Convolutional Neural Network (CNN)**, an architecture specifically designed to \"see\" and recognize patterns, edges, and textures in images through a series of learnable filters. You'll define your model's structure using PyTorch's `nn.Module`, combining several types of layers to create a powerful image classifier.\n",
    "\n",
    "Here's a breakdown of the key layers you'll use:\n",
    "\n",
    "**Convolutional Layer (<code>[nn.Conv2d](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)</code>)**\n",
    "> This is the core building block of a CNN, using learnable filters to scan the image for visual features. The output is a set of \"feature maps\" that highlight where in the image these patterns appear.\n",
    "> * `in_channels`: The number of channels from the previous layer; for the first layer, this is 3 for the RGB color channels.\n",
    "> * `out_channels`: The number of filters the layer will learn, determining the number of output feature maps.\n",
    "> * `kernel_size`: The dimensions of the filter, such as a 3x3 grid that examines a pixel and its immediate neighbors.\n",
    "> * `padding`: Adds a border around the image, allowing the kernel to process edge pixels while preserving the image's dimensions.\n",
    "\n",
    "**ReLU Activation Function (<code>[nn.ReLU](https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html)</code>)**\n",
    "> An activation function that introduces non-linearity by changing all negative values in the feature maps to zero. This helps the model learn more complex patterns.\n",
    "\n",
    "**Max Pooling Layer (<code>[nn.MaxPool2d](https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)</code>)**\n",
    "> This layer downsamples the feature maps by reducing their height and width, which makes the network more efficient. It slides a window over the feature map and keeps only the single largest value from that window, discarding the rest.\n",
    "> * `kernel_size`: The size of the window to perform pooling on, such as a 2x2 area.\n",
    "> * `stride`: The step size the window moves across the image. A stride of 2 with a 2x2 kernel will halve the feature map's dimensions.\n",
    "\n",
    "**Flatten Layer (<code>[nn.Flatten](https://docs.pytorch.org/docs/stable/generated/torch.nn.Flatten.html)</code>)**\n",
    "> A utility layer that unrolls the 2D feature maps into a single 1D vector. This is a necessary step to prepare the data for the fully connected linear layers.\n",
    "\n",
    "**Linear Layer (<code>[nn.Linear](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html)</code>)**\n",
    "> Also known as a fully connected layer, it performs the final classification. It combines the features learned by the convolutional layers into a final prediction.\n",
    "\n",
    "**Dropout Layer (<code>[nn.Dropout](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html)</code>)**\n",
    "> A regularization technique that helps prevent overfitting by randomly setting a fraction of neuron activations to zero during training. This forces the network to learn more robust features instead of relying too heavily on any single pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ea2fd-c5a1-4677-8d3b-02b650e4adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Neural Network model.\n",
    "\n",
    "    The architecture consists of three convolutional blocks followed by two\n",
    "    fully connected layers for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the neural network.\n",
    "\n",
    "        Args:\n",
    "            num_classes: The number of output classes for the final layer.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class (nn.Module)\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Define the first convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the second convolutional block\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the third convolutional block\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the layer to flatten the feature maps\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the fully connected (dense) layers\n",
    "        # Input image is 32x32, after 3 pooling layers: 4x4\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            The output tensor containing the logits for each class.\n",
    "        \"\"\"\n",
    "        # Pass input through the first convolutional block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Pass feature maps through the second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Pass feature maps through the third convolutional block\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass the flattened features through the fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Return the final output logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542552c0-a86d-44f6-b199-6b8f787a0716",
   "metadata": {},
   "source": [
    "With the `SimpleCNN` architecture defined, the next step is to create an instance of the model for your prototype.\n",
    "\n",
    "* First, dynamically determine the number of output classes by checking the length of the class list in your `train_dataset_proto`.\n",
    "* Create an instance of your `SimpleCNN`, passing `num_classes` to its constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70233b-fbfe-4379-9a51-202e10740cec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of classes\n",
    "num_classes = len(train_dataset_proto.classes)\n",
    "\n",
    "# Instantiate the model\n",
    "prototype_model = SimpleCNN(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce265eb4-fae5-4f76-885c-384431c2ad1c",
   "metadata": {},
   "source": [
    "Before you start training, it's very helpful to visualize how the shape of your data changes as it flows through the CNN. This will confirm that your architecture is set up correctly and show you how the spatial dimensions shrink while the number of channels grows with each convolutional block.\n",
    "\n",
    "* Define the `print_data_flow` helper function. \n",
    "    * This function will pass a sample 32x32 color image through your model, layer by layer, printing the tensor's shape at each key step to trace its journey from input to final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a6c97-136c-4b5c-b117-268f4b09464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_flow(model):\n",
    "    \"\"\"\n",
    "    Prints the shape of a tensor as it flows through each layer of the model.\n",
    "\n",
    "    Args:\n",
    "        model: An instance of the PyTorch model to inspect.\n",
    "    \"\"\"\n",
    "    # Create a sample input tensor (batch_size, channels, height, width)\n",
    "    x = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "    # Track the tensor shape at each stage\n",
    "    print(f\"Input shape: \\t\\t{x.shape}\")\n",
    "\n",
    "    # First conv block\n",
    "    x = model.conv1(x)\n",
    "    print(f\"After conv1: \\t\\t{x.shape}\")\n",
    "    x = model.relu1(x)\n",
    "    x = model.pool1(x)\n",
    "    print(f\"After pool1: \\t\\t{x.shape}\")\n",
    "\n",
    "    # Second conv block\n",
    "    x = model.conv2(x)\n",
    "    print(f\"After conv2: \\t\\t{x.shape}\")\n",
    "    x = model.relu2(x)\n",
    "    x = model.pool2(x)\n",
    "    print(f\"After pool2: \\t\\t{x.shape}\")\n",
    "\n",
    "    # Third conv block\n",
    "    x = model.conv3(x)\n",
    "    print(f\"After conv3: \\t\\t{x.shape}\")\n",
    "    x = model.relu3(x)\n",
    "    x = model.pool3(x)\n",
    "    print(f\"After pool3: \\t\\t{x.shape}\")\n",
    "\n",
    "    # Flatten using the model's flatten layer\n",
    "    x = model.flatten(x)\n",
    "    print(f\"After flatten: \\t\\t{x.shape}\")\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = model.fc1(x)\n",
    "    print(f\"After fc1: \\t\\t{x.shape}\")\n",
    "    x = model.relu4(x)\n",
    "    x = model.dropout(x)\n",
    "    x = model.fc2(x)\n",
    "    print(f\"Output shape (fc2): \\t{x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ee742-bddb-44bb-a12b-f6c0e40d56ab",
   "metadata": {},
   "source": [
    "You can now print a summary of your model and trace the data flow to see it in action.\n",
    "\n",
    "* Call your helper function to print the tensor's shape at each step.\n",
    "\n",
    "    * The tensor starts as a `(1, 3, 32, 32)` image. As it passes through the `conv` and `pool` blocks, the number of **channels increases** while the **spatial size is halved** at each step.\n",
    "\n",
    "    * The final `(1, 128, 4, 4)` feature map is **flattened** into a 1D vector to be processed by the linear layers. The model's final **output shape is** `(1, 9)`, providing one score for each of the 9 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557df62-06da-4670-b889-6fea472e118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model's architecture\n",
    "print(prototype_model)\n",
    "\n",
    "# Call the helper function to visualize the data flow\n",
    "print(\"\\n--- Tracing Data Flow ---\")\n",
    "print_data_flow(prototype_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb39602-b5dd-4286-b678-97bab1ef96d1",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With your model defined and the data pipeline prepared, you're ready to set up the training process. This involves initializing a loss function to measure your model's error and an optimizer to update its weights based on that error.\n",
    "\n",
    "### Initialize Loss Function and Optimizer\n",
    "\n",
    "Before starting the training loop, you'll define two key components:\n",
    "\n",
    "* You'll use <code>[nn.CrossEntropyLoss](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)</code>. This is the standard loss function for multi-class classification tasks as it's designed to measure the error when a model has to choose one class from several possibilities.\n",
    "* You'll use the <code>[Adam](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html)</code> optimizer. This is a popular and efficient algorithm that updates the model's weights to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ea397-ac90-40dc-ba0d-0d3a13998de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer for the prototype model\n",
    "optimizer_prototype = optim.Adam(prototype_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b72a061-d927-4977-8279-4b5d5f6362b0",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "\n",
    "* Next, you'll define the `training_loop` function. This function encapsulates the entire process of training and validating your model over multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd7b0a-1435-45f2-a03a-1e0240f238fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_loader, val_loader, loss_function, optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Trains and validates a PyTorch neural network model.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to be trained.\n",
    "        train_loader: DataLoader for the training dataset.\n",
    "        val_loader: DataLoader for the validation dataset.\n",
    "        loss_function: The loss function to use for training.\n",
    "        optimizer: The optimization algorithm.\n",
    "        num_epochs: The total number of epochs to train for.\n",
    "        device: The device (e.g., 'cpu' or 'cuda') to run the training on.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The trained model.\n",
    "        - A list of metrics [train_losses, val_losses, val_accuracies].\n",
    "    \"\"\"\n",
    "    # Move the model to the specified device (CPU or GPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize lists to store training and validation metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Print a message indicating the start of the training process\n",
    "    print(\"--- Training Started ---\")\n",
    "    \n",
    "    # Loop over the specified number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        # Initialize running loss for the current epoch\n",
    "        running_loss = 0.0\n",
    "        # Iterate over batches of data in the training loader\n",
    "        for images, labels in train_loader:\n",
    "            # Move images and labels to the specified device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # Perform a forward pass to get model outputs\n",
    "            outputs = model(images)\n",
    "            # Calculate the loss\n",
    "            loss = loss_function(outputs, labels)\n",
    "            # Perform a backward pass to compute gradients\n",
    "            loss.backward()\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate the training loss for the batch\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "        # Calculate the average training loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        # Append the epoch loss to the list of training losses\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        # Initialize running validation loss and correct predictions count\n",
    "        running_val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Disable gradient calculations for validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over batches of data in the validation loader\n",
    "            for images, labels in val_loader:\n",
    "                # Move images and labels to the specified device\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Perform a forward pass to get model outputs\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Calculate the validation loss for the batch\n",
    "                val_loss = loss_function(outputs, labels)\n",
    "                # Accumulate the validation loss\n",
    "                running_val_loss += val_loss.item() * images.size(0)\n",
    "                \n",
    "                # Get the predicted class labels\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                # Update the total number of samples\n",
    "                total += labels.size(0)\n",
    "                # Update the number of correct predictions\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        # Calculate the average validation loss for the epoch\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        # Append the epoch validation loss to the list\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        # Calculate the validation accuracy for the epoch\n",
    "        epoch_accuracy = 100.0 * correct / total\n",
    "        # Append the epoch accuracy to the list\n",
    "        val_accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        # Print the metrics for the current epoch\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_accuracy:.2f}%\")\n",
    "        \n",
    "    # Print a message indicating the end of the training process\n",
    "    print(\"--- Finished Training ---\")\n",
    "    \n",
    "    # Consolidate all metrics into a single list\n",
    "    metrics = [train_losses, val_losses, val_accuracies]\n",
    "    \n",
    "    # Return the trained model and the collected metrics\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdd57b7-911b-4c00-85bf-26bf44ddc4ba",
   "metadata": {},
   "source": [
    "With all the components in place, you're ready to start training.\n",
    "\n",
    "* Run the `training_loop` function with your prototype model (for 9 classes), its corresponding data loaders, the loss function, and the optimizer. \n",
    "* You'll train for `15 epochs`, and the function will return the trained model along with the collected performance metrics.\n",
    "* After training is complete, you'll use the `plot_training_metrics` helper function to visualize the training and validation loss, along with the validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ed632-494b-4368-a476-499a89e3d921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the training process by calling the training loop function\n",
    "trained_proto_model, training_metrics_proto = training_loop(\n",
    "    model=prototype_model, \n",
    "    train_loader=train_loader_proto, \n",
    "    val_loader=val_loader_proto, \n",
    "    loss_function=loss_function, \n",
    "    optimizer=optimizer_prototype, \n",
    "    num_epochs=15, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Visualize the training metrics (loss and accuracy)\n",
    "print(\"\\n--- Training Plots ---\\n\")\n",
    "helper_utils.plot_training_metrics(training_metrics_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c98106-3b56-4bde-a5b6-88f6fe2be475",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "Excellent work! The prototype model is trained, and the results look very promising. Achieving a validation accuracy of over **75%** on the 9-class subset is a great result and confirms that your CNN architecture is well-suited for this task.\n",
    "\n",
    "This successful prototype gives you the green light to move forward with the next phase: training a full-scale model on all 15 classes for the butterfly house. But before you do, it's helpful to perform one last qualitative check to see how your model \"thinks.\"\n",
    "\n",
    "### Visualizing Predictions\n",
    "\n",
    "While the plots show your model's overall performance, looking at individual predictions provides a more intuitive feel for its strengths and weaknesses. You can now use a helper function to see your model in action, visualizing its predictions on random images from the validation set. This will show you concrete examples of where it succeeds and where it might be making mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958045c8-5d4d-4249-b596-06f763e8d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model predictions on a sample of validation images\n",
    "helper_utils.visualise_predictions(\n",
    "    model=trained_proto_model, \n",
    "    data_loader=val_loader_proto, \n",
    "    device=device, \n",
    "    grid=(3, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602d397-9fe5-4872-baff-e28e3a78375a",
   "metadata": {},
   "source": [
    "## Scaling Up: Training the Full Model \n",
    "\n",
    "The prototype was a success! Now it's time to train the final model for the butterfly house app. You'll repeat the same steps as before, but this time using the full, more challenging dataset of **15 classes**.\n",
    "\n",
    "* First, create a new list containing all 15 target classes.\n",
    "* Use the `load_cifar100_subset` helper function again to create the new training and validation datasets based on this full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fee28-7f72-4add-8e6a-df1ee64e28de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the full class list.\n",
    "all_target_classes = [\n",
    "    # Flowers\n",
    "    'orchid', 'poppy', 'rose', 'sunflower', 'tulip',\n",
    "    # Mammals\n",
    "    'fox', 'porcupine', 'possum', 'raccoon', 'skunk',\n",
    "    # Insects\n",
    "    'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'\n",
    "]\n",
    "\n",
    "# Load the full datasets.\n",
    "train_dataset, val_dataset = helper_utils.load_cifar100_subset(all_target_classes, train_transform, val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ea666-3159-47d3-90ee-969ad888c3aa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Wrap your new 15-class datasets in `DataLoader` instances, using the same `batch_size=64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b8ad4-71e4-4c4b-b6b1-14b5d42908f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a data loader for the training set, with shuffling enabled\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create a data loader for the validation set, without shuffling\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44507470-ab6f-49a1-a40d-43954ad93f9d",
   "metadata": {},
   "source": [
    "* Display a sample of images from your new 15-class training set to confirm it has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6a7d8-d7ec-4178-a396-c6768439a936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize a 3x5 grid of random training images\n",
    "helper_utils.visualise_images(train_dataset, grid=(3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7284c-78c4-4c23-9b77-19cedab4029d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Create a new instance of your `SimpleCNN` model, this time configured for all **15 classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f022df-ae49-4889-bb5f-f91d7cb15b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of classes\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Instantiate the full model\n",
    "model = SimpleCNN(num_classes)\n",
    "\n",
    "# Print the model's architecture (notice, it now has 15 output classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1371f0e7-467d-4fbe-938a-606d3201a4dc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Create a new `Adam` optimizer for your full 15-class model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22055b29-f3e7-4aa7-a211-e8817259bd82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizer for the full model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03844faf-95ea-466b-a187-194e932b7218",
   "metadata": {},
   "source": [
    "* Call the `training_loop` to train your 15-class model for `25 epochs`. The `plot_training_metrics` function will then immediately visualize the loss and accuracy curves from this final training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80f8dd-431d-40a4-9dbe-7708c8c25af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the training process for the full model on all 15 classes\n",
    "trained_model, training_metrics = training_loop(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    loss_function=loss_function, \n",
    "    optimizer=optimizer, \n",
    "    num_epochs=25, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Visualize the training metrics for the full model\n",
    "print(\"\\n--- Training Plots ---\\n\")\n",
    "helper_utils.plot_training_metrics(training_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196c5c5-670c-40a5-8763-e931bbe13e5d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "After training the full model, you can analyze the results. But wait, something isn't right here. Your prototype model trained successfully, showing steady improvement. However, the performance on the full 15-class dataset seems to have hit a wall. What happened?\n",
    "\n",
    "A close look at the plots reveals the problem. While the **Training Loss** consistently decreases, the **Validation Loss** drops for a while and then begins to rise and fluctuate. At the same time, the **Validation Accuracy** gets stuck, plateauing without making further significant progress. This is a classic case of **overfitting**.\n",
    "\n",
    "Overfitting occurs when a model learns the training data *too well*, including its noise and specific quirks, instead of the general, underlying patterns that would help it perform on new, unseen data. The widening gap between your training and validation loss is a clear sign your model is memorizing the training set instead of learning to **generalize**.\n",
    "\n",
    "You might wonder why this happened now and not with the 9-class prototype. The reason is the significant increase in **task complexity**. Distinguishing between 15 classes is much harder than 9, requiring the model to learn more subtle features. Faced with this harder challenge, your powerful CNN model found an easier path to lowering the training loss: it started to memorize the training data instead of learning to generalize.\n",
    "\n",
    "This overfitting problem presents a realistic challenge, similar to what you'd encounter in a real-world project. In this module's graded assignment, you'll tackle this issue by making several updates to your entire pipeline to see if you can improve the model's ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0d712-5ffa-45f3-a9c9-4306f4bd42ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Optional: Uncomment and run this cell to see the predictions made by the full model\n",
    "\n",
    "# helper_utils.visualise_predictions(\n",
    "#     model=trained_model, \n",
    "#     data_loader=val_loader, \n",
    "#     device=device, \n",
    "#     grid=(3, 5)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f84bc3e-0d31-4ab9-99a3-2eb2f79940b8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing the lab! You have successfully navigated the entire machine learning pipeline, from data preparation to building, training, and analyzing your very own Convolutional Neural Network.\n",
    "\n",
    "You've put theory into practice by building a CNN architecture capable of learning complex visual patterns. More importantly, you've experienced a realistic, iterative development workflow by first creating a successful prototype and then scaling up to a more complex model. This process led you to encounter and diagnose overfitting, a fundamental challenge that every machine learning practitioner must learn to solve.\n",
    "\n",
    "The skills you've developed here have prepared you for the next step. You've identified the problem, and in the graded assignment, you'll get to solve it. Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
